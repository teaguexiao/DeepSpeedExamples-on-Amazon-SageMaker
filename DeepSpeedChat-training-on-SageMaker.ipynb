{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b666ee7",
   "metadata": {},
   "source": [
    "# Running DeepSpeedChaton Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae555b",
   "metadata": {},
   "source": [
    "This is a sample code to run stanford_alpaca on Amazon SageMaker, for demo or research use only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5898f9b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.186.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.187.0.tar.gz (886 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.2/886.2 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.28.41)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.25.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.187.0-py2.py3-none-any.whl size=1186949 sha256=528702d588be2c57879c15c3330d4f1f34f40cffe67b855b599b68e431fa3fdb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/21/d2/2b/ed240a9d3084c5b4e3e7fd2ca8f9c11659bb98f0b87b6b1ca3\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.186.0\n",
      "    Uninstalling sagemaker-2.186.0:\n",
      "      Successfully uninstalled sagemaker-2.186.0\n",
      "Successfully installed sagemaker-2.187.0\n"
     ]
    }
   ],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6387eff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-427169985960\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "print(sagemaker_default_bucket)\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88cf40",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081c5b",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a9df6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0239c2d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371d4cdba944cbfa8905f5b430a0c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a17cee579ec4442a4a753fb80eede1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c7e45565f04b199a5980f2756814e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)02239/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcff8bafca54d71bb1586bbaacb50b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)d1402239/config.json:   0%|          | 0.00/554 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfc84d2cfdd4391af584860f57faac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b59ac8852cb4b5694f88d0ef3e97904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d145d172334808a3f84a971a167fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "156109d4954e4a3b920f42d156176b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218c88d568e74417832423793075715b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0048c55e9afc451b819caeab40f7f794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "#model_name = \"TheBloke/Llama-2-13B-fp16\"\n",
    "model_name = \"TheBloke/Llama-2-7B-fp16\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be704ef5",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd09c171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\n",
      "9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
      "400e3de6ffc3884ec3c158a046f6a04da00ef3ca\n",
      "4cad8f6ae78aa8cf03e750a83a2b42f7ecc03c54\n",
      "422bfd1b447eb0182551ac305fbc03e299e7f37a981317590e3a4362b6708179\n",
      "77b916244c2215098baa37dbdee4037698fc31744dff92209d3142214294fce9\n",
      "db7264b24cac7a39947bb5fc02fe5c2d7ac9eaf4\n",
      "67a2e09f9d8b5e85eca24e88aa5c0fb465bbafd6\n",
      "d85ba6cb6820b01226ef8bd40b46bb489041c6a8\n",
      "89c31b8e044c6d3589cd86fe3be48558d8bab8fc\n",
      "tokenizer.model\n",
      "pytorch_model-00002-of-00002.bin\n",
      "tokenizer.json\n",
      "pytorch_model.bin.index.json\n",
      "generation_config.json\n",
      "pytorch_model-00001-of-00002.bin\n",
      "special_tokens_map.json\n",
      "tokenizer_config.json\n",
      "config.json\n",
      "./model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/config.json\n",
      "./model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d6ee3-c5f1-4f5a-877e-34174ba7f4e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download s5cmd and put it in src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3f989-f35d-43a3-a788-55aad789b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz\n",
    "\n",
    "!tar -xvf s5cmd_2.2.2_Linux-64bit.tar.gz\n",
    "\n",
    "!mv s5cmd src/\n",
    "!rm -rf s5cmd_2.2.2_Linux-64bit.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66131c-5dd3-41b5-8614-6e45e9fed77c",
   "metadata": {},
   "source": [
    "**Rewrite upload module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5716ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/generation_config.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/generation_config.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/config.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/config.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/pytorch_model.bin.index.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model.bin.index.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/tokenizer_config.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer_config.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/special_tokens_map.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/special_tokens_map.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/tokenizer.model s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer.model\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/tokenizer.json s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer.json\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/pytorch_model-00002-of-00002.bin s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model-00002-of-00002.bin\n",
      "cp model/models--TheBloke--Llama-2-7B-fp16/snapshots/ba2306439903c2ebf7d09970a973ef44d1402239/pytorch_model-00001-of-00002.bin s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model-00001-of-00002.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x src/s5cmd\n",
    "src/s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/Llama-2-7B-fp16/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c0a9a",
   "metadata": {},
   "source": [
    "## Prepare a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48ff3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "#From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "FROM nvcr.io/nvidia/pytorch:23.02-py3\n",
    "RUN pip3 install sagemaker-training\n",
    "\n",
    "#Remove Cuda 11.8\n",
    "#RUN apt-get -y purge cuda*\n",
    "#RUN apt-get -y autoremove\n",
    "#RUN apt-get -y autoclean\n",
    "#RUN rm -rf /usr/local/cuda*\n",
    "\n",
    "#install Cuda 12\n",
    "#RUN wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
    "#RUN mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "#RUN wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb\n",
    "#RUN dpkg -i cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb\n",
    "#RUN cp /var/cuda-repo-ubuntu2004-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "#RUN apt-get update\n",
    "#RUN apt-get -y install cuda\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN update-alternatives --display cuda\n",
    "RUN update-alternatives --auto cuda\n",
    "RUN python3 -m pip uninstall -y deepspeed \n",
    "#RUN python3 -m pip install pytorch-lightning==1.9.0\n",
    "\n",
    "#From requirement.txt\n",
    "RUN python3 -m pip install datasets>=2.8.0\n",
    "RUN python3 -m pip install sentencepiece>=0.1.97\n",
    "RUN python3 -m pip install protobuf==3.20.3\n",
    "RUN python3 -m pip install accelerate>=0.15.0\n",
    "RUN python3 -m pip install torch>=1.12.0\n",
    "RUN python3 -m pip install deepspeed>=0.9.0\n",
    "RUN python3 -m pip install transformers==4.31.0\n",
    "#RUN python3 -m pip install transformers>=4.31.0,!=4.33.2\n",
    "RUN python3 -m pip install tensorboard\n",
    "\n",
    "\n",
    "## Install transfomers version which support LLaMaTokenizer\n",
    "#RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42e8b4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-east-1\n",
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d1962",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1717f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-llama2-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a814f9d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  113.5MB\n",
      "Step 1/17 : FROM nvcr.io/nvidia/pytorch:23.02-py3\n",
      " ---> 7c3375e220ea\n",
      "Step 2/17 : RUN pip3 install sagemaker-training\n",
      " ---> Using cache\n",
      " ---> c26db60cc80d\n",
      "Step 3/17 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> daf94321439e\n",
      "Step 4/17 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> bd8d2fd7a9a1\n",
      "Step 5/17 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> ff65d34542c3\n",
      "Step 6/17 : RUN update-alternatives --display cuda\n",
      " ---> Using cache\n",
      " ---> e04ac8360543\n",
      "Step 7/17 : RUN update-alternatives --auto cuda\n",
      " ---> Using cache\n",
      " ---> 6c2f3c2edc0f\n",
      "Step 8/17 : RUN python3 -m pip uninstall -y deepspeed\n",
      " ---> Using cache\n",
      " ---> f14a5a8f5df8\n",
      "Step 9/17 : RUN python3 -m pip install datasets>=2.8.0\n",
      " ---> Using cache\n",
      " ---> 6065025ed0e2\n",
      "Step 10/17 : RUN python3 -m pip install sentencepiece>=0.1.97\n",
      " ---> Using cache\n",
      " ---> e9186d65012f\n",
      "Step 11/17 : RUN python3 -m pip install protobuf==3.20.3\n",
      " ---> Using cache\n",
      " ---> ee7e2bcf85e7\n",
      "Step 12/17 : RUN python3 -m pip install accelerate>=0.15.0\n",
      " ---> Using cache\n",
      " ---> 59718b1225de\n",
      "Step 13/17 : RUN python3 -m pip install torch>=1.12.0\n",
      " ---> Using cache\n",
      " ---> 69ef1933c40d\n",
      "Step 14/17 : RUN python3 -m pip install deepspeed>=0.9.0\n",
      " ---> Using cache\n",
      " ---> 8f03265f3d9c\n",
      "Step 15/17 : RUN python3 -m pip install transformers==4.31.0\n",
      " ---> Running in 88b6e07ca1e3\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (0.17.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (1.22.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (1.26.13)\n",
      "Installing collected packages: tokenizers, safetensors, transformers\n",
      "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.31.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 88b6e07ca1e3\n",
      " ---> 5d08c2e8c06c\n",
      "Step 16/17 : RUN python3 -m pip install tensorboard\n",
      " ---> Running in 1177e5c7208e\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.28.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (65.5.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.22.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 1177e5c7208e\n",
      " ---> 94ce99c436a1\n",
      "Step 17/17 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in 8b31d820ebb6\n",
      "Removing intermediate container 8b31d820ebb6\n",
      " ---> add7e22514cd\n",
      "Successfully built add7e22514cd\n",
      "Successfully tagged sagemaker-llama2-13b:latest\n",
      "The push refers to repository [427169985960.dkr.ecr.us-east-1.amazonaws.com/sagemaker-llama2-13b]\n",
      "462fa0b917db: Preparing\n",
      "965fad35d1a3: Preparing\n",
      "21a26dbd9837: Preparing\n",
      "c7f3e28be5a6: Preparing\n",
      "1c23b1f8974a: Preparing\n",
      "b05104af43a8: Preparing\n",
      "00ce1d46ae63: Preparing\n",
      "a80034093396: Preparing\n",
      "1b332cf0a00c: Preparing\n",
      "81a23279aecf: Preparing\n",
      "777f03a73d92: Preparing\n",
      "c395de67fa59: Preparing\n",
      "847e4cb476a3: Preparing\n",
      "362395390e07: Preparing\n",
      "e463f1e12549: Preparing\n",
      "90cfa3fde55b: Preparing\n",
      "b05104af43a8: Waiting\n",
      "05be79be3d00: Preparing\n",
      "1b332cf0a00c: Waiting\n",
      "fa19871c9d97: Preparing\n",
      "9c08be9a4df4: Preparing\n",
      "81a23279aecf: Waiting\n",
      "00ce1d46ae63: Waiting\n",
      "97cfdcfee476: Preparing\n",
      "27d771c824cd: Preparing\n",
      "a80034093396: Waiting\n",
      "777f03a73d92: Waiting\n",
      "38f523d14ea6: Preparing\n",
      "e463f1e12549: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "c395de67fa59: Waiting\n",
      "90cfa3fde55b: Waiting\n",
      "0bb8fce9600a: Preparing\n",
      "4963cc779521: Preparing\n",
      "847e4cb476a3: Waiting\n",
      "05be79be3d00: Waiting\n",
      "362395390e07: Waiting\n",
      "fc6980be3345: Preparing\n",
      "1be3b48a2dd2: Preparing\n",
      "fa19871c9d97: Waiting\n",
      "129b6b66762e: Preparing\n",
      "28a0898660e6: Preparing\n",
      "178f233b6e82: Preparing\n",
      "9c08be9a4df4: Waiting\n",
      "3629a41d267e: Preparing\n",
      "97cfdcfee476: Waiting\n",
      "3c9606098737: Preparing\n",
      "27d771c824cd: Waiting\n",
      "aea7572c6e7c: Preparing\n",
      "28e5b649aa69: Preparing\n",
      "b3eb3f6a881e: Preparing\n",
      "ad36af83523b: Preparing\n",
      "93e86c739007: Preparing\n",
      "2eb589fcc194: Preparing\n",
      "7e114556b94d: Preparing\n",
      "b8390e51490f: Preparing\n",
      "d76bc5715d79: Preparing\n",
      "65f8041c117e: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "2f01e60f0b15: Preparing\n",
      "595350e08a5c: Preparing\n",
      "cf543fc4fccf: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "ad7dc7a9e053: Preparing\n",
      "8067ce044f51: Preparing\n",
      "f8762797e65a: Preparing\n",
      "9c6a2d006077: Preparing\n",
      "de38e00f8e6c: Preparing\n",
      "122ac7e12cc3: Preparing\n",
      "871ecd890743: Preparing\n",
      "0d2b276997c6: Preparing\n",
      "df62fb1402ea: Preparing\n",
      "75e6b8a32bd3: Preparing\n",
      "1f20c057b484: Preparing\n",
      "dd2bc8e06034: Preparing\n",
      "d543b8cad89e: Preparing\n",
      "38f523d14ea6: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "0bb8fce9600a: Waiting\n",
      "4963cc779521: Waiting\n",
      "fc6980be3345: Waiting\n",
      "1be3b48a2dd2: Waiting\n",
      "129b6b66762e: Waiting\n",
      "cf543fc4fccf: Waiting\n",
      "ad7dc7a9e053: Waiting\n",
      "28a0898660e6: Waiting\n",
      "178f233b6e82: Waiting\n",
      "8067ce044f51: Waiting\n",
      "f8762797e65a: Waiting\n",
      "75e6b8a32bd3: Waiting\n",
      "3629a41d267e: Waiting\n",
      "9c6a2d006077: Waiting\n",
      "3c9606098737: Waiting\n",
      "de38e00f8e6c: Waiting\n",
      "122ac7e12cc3: Waiting\n",
      "1f20c057b484: Waiting\n",
      "dd2bc8e06034: Waiting\n",
      "aea7572c6e7c: Waiting\n",
      "871ecd890743: Waiting\n",
      "0d2b276997c6: Waiting\n",
      "b8390e51490f: Waiting\n",
      "df62fb1402ea: Waiting\n",
      "28e5b649aa69: Waiting\n",
      "595350e08a5c: Waiting\n",
      "b3eb3f6a881e: Waiting\n",
      "65f8041c117e: Waiting\n",
      "2f01e60f0b15: Waiting\n",
      "d76bc5715d79: Waiting\n",
      "ad36af83523b: Waiting\n",
      "2eb589fcc194: Waiting\n",
      "7e114556b94d: Waiting\n",
      "93e86c739007: Waiting\n",
      "21a26dbd9837: Layer already exists\n",
      "c7f3e28be5a6: Layer already exists\n",
      "1c23b1f8974a: Layer already exists\n",
      "b05104af43a8: Layer already exists\n",
      "00ce1d46ae63: Layer already exists\n",
      "a80034093396: Layer already exists\n",
      "1b332cf0a00c: Layer already exists\n",
      "81a23279aecf: Layer already exists\n",
      "777f03a73d92: Layer already exists\n",
      "c395de67fa59: Layer already exists\n",
      "847e4cb476a3: Layer already exists\n",
      "362395390e07: Layer already exists\n",
      "90cfa3fde55b: Layer already exists\n",
      "e463f1e12549: Layer already exists\n",
      "05be79be3d00: Layer already exists\n",
      "fa19871c9d97: Layer already exists\n",
      "9c08be9a4df4: Layer already exists\n",
      "97cfdcfee476: Layer already exists\n",
      "27d771c824cd: Layer already exists\n",
      "38f523d14ea6: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "0bb8fce9600a: Layer already exists\n",
      "4963cc779521: Layer already exists\n",
      "fc6980be3345: Layer already exists\n",
      "129b6b66762e: Layer already exists\n",
      "462fa0b917db: Pushed\n",
      "1be3b48a2dd2: Layer already exists\n",
      "28a0898660e6: Layer already exists\n",
      "178f233b6e82: Layer already exists\n",
      "3629a41d267e: Layer already exists\n",
      "3c9606098737: Layer already exists\n",
      "aea7572c6e7c: Layer already exists\n",
      "28e5b649aa69: Layer already exists\n",
      "b3eb3f6a881e: Layer already exists\n",
      "ad36af83523b: Layer already exists\n",
      "7e114556b94d: Layer already exists\n",
      "93e86c739007: Layer already exists\n",
      "2eb589fcc194: Layer already exists\n",
      "b8390e51490f: Layer already exists\n",
      "d76bc5715d79: Layer already exists\n",
      "65f8041c117e: Layer already exists\n",
      "2f01e60f0b15: Layer already exists\n",
      "595350e08a5c: Layer already exists\n",
      "cf543fc4fccf: Layer already exists\n",
      "ad7dc7a9e053: Layer already exists\n",
      "8067ce044f51: Layer already exists\n",
      "f8762797e65a: Layer already exists\n",
      "9c6a2d006077: Layer already exists\n",
      "de38e00f8e6c: Layer already exists\n",
      "871ecd890743: Layer already exists\n",
      "0d2b276997c6: Layer already exists\n",
      "df62fb1402ea: Layer already exists\n",
      "122ac7e12cc3: Layer already exists\n",
      "75e6b8a32bd3: Layer already exists\n",
      "1f20c057b484: Layer already exists\n",
      "dd2bc8e06034: Layer already exists\n",
      "d543b8cad89e: Layer already exists\n",
      "965fad35d1a3: Pushed\n",
      "latest: digest: sha256:9e95f79d9629ad5d82742cc53bc2247e8d4d1dcf3f4e057640355534216b1a60 size: 12943\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} . \n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f07bc7-a676-4e9d-9bfb-1f5f703d8a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/DeepSpeedExamples/applications/DeepSpeed-Chat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8716141c-bb16-4a7a-95e1-ca0471d3b328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl stop docker.service\n",
      "Failed to stop docker.service: The name org.freedesktop.PolicyKit1 was not provided by any .service files\n",
      "See system logs and 'systemctl status docker.service' for details.\n",
      "Warning: Stopping docker.service, but it can still be activated by:\n",
      "  docker.socket\n",
      "Redirecting to /bin/systemctl start docker.service\n",
      "Failed to start docker.service: The name org.freedesktop.PolicyKit1 was not provided by any .service files\n",
      "See system logs and 'systemctl status docker.service' for details.\n"
     ]
    }
   ],
   "source": [
    "!service docker stop\n",
    "!service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4edf0d",
   "metadata": {},
   "source": [
    "### Generate training entrypoint script\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc847d",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca84c5-cdd9-4fa7-8592-3baa76316749",
   "metadata": {},
   "source": [
    "## Modify some files due to some bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406152c7-5485-4ed6-961e-ce227f25c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify DeepSpeedChat-on-Amazon-SageMaker/src/DeepSpeed-Chat/training/utils/utils.py\n",
    "\n",
    "#Find below function\n",
    "def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
    "    if os.path.exists(model_name_or_path):\n",
    "        # Locally tokenizer loading has some issue, so we need to force download\n",
    "        model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "        if os.path.exists(model_json):\n",
    "            model_json_file = json.load(open(model_json))\n",
    "            model_name = model_json_file[\"_name_or_path\"]\n",
    "            tokenizer = get_tokenizer(model_name,\n",
    "                                      fast_tokenizer=fast_tokenizer)\n",
    "    else:\n",
    "        tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                  fast_tokenizer=fast_tokenizer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "#####\n",
    "#Replace it into\n",
    "\n",
    "def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
    "    tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                  fast_tokenizer=fast_tokenizer)\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a14a6-48f9-423e-a2a6-37f6c635a4a1",
   "metadata": {},
   "source": [
    "## Make some sample training Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2f83c-b2be-4135-a2ec-801967f7d36e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./dataset\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"Dahoas/synthetic-instruct-gptj-pairwise\"#\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cad13b-74d9-41b6-8ebc-85ee9db1ed5b",
   "metadata": {},
   "source": [
    "### Modify training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b273426-d6f7-43bf-98bd-fbcf6e1567b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/Llama-2-7B-fp16/* /tmp/llama-2-7B-fp16/\n",
    "#./s5cmd sync s3://$MODEL_S3_BUCKET/llama-7b-hf/* /tmp/llama-7b-hf/\n",
    "\n",
    "\n",
    "cd DeepSpeed-Chat/training/step1_supervised_finetuning/\n",
    "\n",
    "#Fix huggingface issue\n",
    "#pip install --upgrade huggingface_hub\n",
    "#huggingface-cli login --token hf_xxxx\n",
    "\n",
    "\n",
    "#bash training_scripts/opt/single_gpu/run_1.3b.sh\n",
    "#!/bin/bash\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# DeepSpeed Team\n",
    "OUTPUT=$1\n",
    "ZERO_STAGE=$2\n",
    "if [ \"$OUTPUT\" == \"\" ]; then\n",
    "    OUTPUT=./output_step1_llama2_7b_sft_demo\n",
    "fi\n",
    "if [ \"$ZERO_STAGE\" == \"\" ]; then\n",
    "    ZERO_STAGE=3\n",
    "fi\n",
    "mkdir -p $OUTPUT\n",
    "\n",
    "#if you are using standard dataset on HF, use main.py\n",
    "#if you are using custom dataset, use main_sft.py\n",
    "\n",
    "deepspeed main_sft.py \\\n",
    "   --data_path local/jsonfile \\\n",
    "   --data_split 10,0,0 \\\n",
    "   --model_name_or_path /tmp/llama-2-7B-fp16/ \\\n",
    "   --per_device_train_batch_size 2 \\\n",
    "   --per_device_eval_batch_size 2 \\\n",
    "   --max_seq_len 4096 \\\n",
    "   --learning_rate 9.65e-6 \\\n",
    "   --weight_decay 0. \\\n",
    "   --num_train_epochs 2  \\\n",
    "   --gradient_accumulation_steps 1 \\\n",
    "   --lr_scheduler_type cosine \\\n",
    "   --num_warmup_steps 0 \\\n",
    "   --seed 1234 \\\n",
    "   --gradient_checkpointing \\\n",
    "   --zero_stage $ZERO_STAGE \\\n",
    "   --deepspeed \\\n",
    "   --output_dir $OUTPUT \\\n",
    "   --print_loss \\\n",
    "   --offload\n",
    "\n",
    "#   --model_name_or_path TheBloke/Llama-2-13B-fp16 \\\n",
    "#   --model_name_or_path /tmp/Llama-2-13B-fp16/ \\\n",
    "#&> $OUTPUT/training.log\n",
    "\n",
    "#set the timer for stopping the training job\n",
    "#sleep 900\n",
    "#kill \"$!\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#Upload the trained models to S3\n",
    "./s5cmd sync $OUTPUT s3://$MODEL_S3_BUCKET/deepspeedchat-finetuned-llama-2-7B-fp16-20230928\n",
    "\n",
    "#s3://$MODEL_S3_BUCKET/RWKV/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f06196b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'427169985960.dkr.ecr.us-east-1.amazonaws.com/sagemaker-llama2-13b:latest'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434d18",
   "metadata": {},
   "source": [
    "**The modified training script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbcf5",
   "metadata": {},
   "source": [
    "Everything is ready, let's launch the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486913ec",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5922e265-c0b8-445a-b29e-af47ce9a493f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod: cannot access ‘lost+found’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod 777 lost+found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e57792db-5f46-499c-bc2d-a7c6252ec2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo chmod -R 777 ./src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b199e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 16:01:36 Starting - Starting the training job...\n",
      "2023-10-10 16:01:41 Downloading - Downloading input data......\n",
      "2023-10-10 16:02:47 Training - Downloading the training image......\n",
      "2023-10-10 16:03:54 Training - Training image download completed. Training in progress....\u001b[34m=============\u001b[0m\n",
      "\u001b[34m== PyTorch ==\u001b[0m\n",
      "\u001b[34m=============\u001b[0m\n",
      "\u001b[34mNVIDIA Release 23.02 (build 53420872)\u001b[0m\n",
      "\u001b[34mPyTorch Version 1.14.0a0+44dac51\u001b[0m\n",
      "\u001b[34mContainer image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2014-2023 Facebook Inc.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2013 NYU                      (Clement Farabet)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2015      Google Inc.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2015      Yangqing Jia\u001b[0m\n",
      "\u001b[34mCopyright (c) 2013-2016 The Caffe contributors\u001b[0m\n",
      "\u001b[34mAll rights reserved.\u001b[0m\n",
      "\u001b[34mVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:20,921 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:20,991 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:21,060 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:21,071 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-10-10-16-01-29-330/source/sourcedir.tar.gz\",\"module_name\":\"train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train.sh \"\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:21,072 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2023-10-10 16:04:21,072 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/generation_config.json /tmp/llama-2-7B-fp16/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer_config.json /tmp/llama-2-7B-fp16/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/config.json /tmp/llama-2-7B-fp16/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/special_tokens_map.json /tmp/llama-2-7B-fp16/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model.bin.index.json /tmp/llama-2-7B-fp16/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer.model /tmp/llama-2-7B-fp16/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/tokenizer.json /tmp/llama-2-7B-fp16/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model-00002-of-00002.bin /tmp/llama-2-7B-fp16/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-7B-fp16/pytorch_model-00001-of-00002.bin /tmp/llama-2-7B-fp16/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:02,228] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:07,035] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:07,035] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main_sft.py --data_path local/jsonfile --data_split 10,0,0 --model_name_or_path /tmp/llama-2-7B-fp16/ --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --max_seq_len 4096 --learning_rate 9.65e-6 --weight_decay 0. --num_train_epochs 2 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --gradient_checkpointing --zero_stage 3 --deepspeed --output_dir ./output_step1_llama2_7b_sft_demo --print_loss --offload\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:08,571] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:163:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:12,798] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,074] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,106] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,109] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,161] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,207] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,208] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,208] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:15,210] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,824] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,884] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,884] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,889] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,914] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,958] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:19,980] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:20,009] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:20,023] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:05:25,597] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.68s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.72s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.98s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.05s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.06s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.08s/it]#015Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.12s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.54s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.01s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.54s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.02s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.53s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.05s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.53s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.54s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.55s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.55s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.55s/it]#015Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.09s/it]\u001b[0m\n",
      "\u001b[34m#015Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]#015Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 9880.57it/s]\u001b[0m\n",
      "\u001b[34m#015Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]#015Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 2038.05it/s]\u001b[0m\n",
      "\u001b[34m#015Generating train split: 0 examples [00:00, ? examples/s]#015Generating train split: 1931 examples [00:00, 53762.06 examples/s]\u001b[0m\n",
      "\u001b[34m#015Generating eval split: 0 examples [00:00, ? examples/s]#015Generating eval split: 60 examples [00:00, 23126.10 examples/s]\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu120/cpu_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu120/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o \u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.777954578399658 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.796601057052612 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.827604055404663 secondsTime to load cpu_adam op: 30.83167004585266 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.79829978942871 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...Loading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.79061532020569 secondsTime to load cpu_adam op: 30.830390453338623 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 30.806519985198975 seconds\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,378] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34mAdam Optimizer #0 is created with AVX512 arithmetic capability.\u001b[0m\n",
      "\u001b[34mConfig: alpha=0.000010, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,421] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,422] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,422] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,436] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,701] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,702] [INFO] [utils.py:804:see_memory_usage] MA 0.55 GB         Max_MA 1.1 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,702] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 60.85 GB, percent = 5.4%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,704] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,704] [INFO] [stage3.py:127:__init__] Prefetch bucket size 30000000\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,918] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,919] [INFO] [utils.py:804:see_memory_usage] MA 0.55 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:41,919] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 60.85 GB, percent = 5.4%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,217] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,218] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.55 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,218] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 61.29 GB, percent = 5.5%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,435] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,436] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:42,436] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 61.29 GB, percent = 5.5%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,589] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,592] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,593] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 94.93 GB, percent = 8.5%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,992] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,993] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:51,993] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 91.15 GB, percent = 8.1%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:52,650] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:52,650] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:52,650] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 94.29 GB, percent = 8.4%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:53,407] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:53,432] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:53,433] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 134.43 GB, percent = 12.0%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:56,547] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:56,547] [INFO] [utils.py:804:see_memory_usage] MA 0.06 GB         Max_MA 0.06 GB         CA 1.78 GB         Max_CA 2 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:56,548] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 169.74 GB, percent = 15.1%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:56,548] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,614] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,614] [INFO] [utils.py:804:see_memory_usage] MA 0.99 GB         Max_MA 1.48 GB         CA 2.95 GB         Max_CA 3 GB \u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,615] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 182.44 GB, percent = 16.3%\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,615] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,615] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,615] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f3ab826e9a0>\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[9.65e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,616] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3a8258c100>\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,617] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   steps_per_print .............. 10\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   train_batch_size ............. 16\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-10-10 16:06:57,618] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 100\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": false, \n",
      "        \"output_path\": \"step1_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step1_model_tensorboard\"\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Evaluating perplexity, Epoch 0/2 *****\u001b[0m\n",
      "\u001b[34mppl: nan\u001b[0m\n",
      "\u001b[34mBeginning of Epoch 1/2, Total Micro Batches 121\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 4, loss = 3.0372202396392822\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 2, loss = 2.2768826484680176\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 6, loss = 2.171759843826294\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 7, loss = 2.7143075466156006\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 5, loss = 1.9603859186172485\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 3, loss = 2.1011221408843994\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 1, loss = 1.9640713930130005\u001b[0m\n",
      "\u001b[34mEpoch: 0, Step: 0, Rank: 0, loss = 1.893707513809204\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'DeepSpeedChat-SageMaker-Training'\n",
    "\n",
    "\n",
    "#instance_type = 'ml.p3dn.24xlarge'\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "#instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      keep_alive_period_in_seconds=3600 #make sure you have quota for SageMaker warm pool, it's easier for debuging\n",
    "                      )\n",
    "\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb8296-2eee-4bee-bb97-8417dc5af1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
