{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b666ee7",
   "metadata": {},
   "source": [
    "# Running DeepSpeedChaton Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae555b",
   "metadata": {},
   "source": [
    "This is a sample code to run stanford_alpaca on Amazon SageMaker, for demo or research use only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5898f9b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.183.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.186.0.tar.gz (885 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m885.1/885.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.28.41)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.25.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.186.0-py2.py3-none-any.whl size=1185876 sha256=afa4636eb919e0581a35d1007fae8f7e09c7c7364c503a60eb713f8866af6a86\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/1a/17/d8/4b14865700f71649882581649873bea486d2a350d961ea9cfb\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.183.0\n",
      "    Uninstalling sagemaker-2.183.0:\n",
      "      Successfully uninstalled sagemaker-2.183.0\n",
      "Successfully installed sagemaker-2.186.0\n"
     ]
    }
   ],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6387eff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-427169985960\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "print(sagemaker_default_bucket)\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88cf40",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081c5b",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a9df6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Obtaining dependency information for huggingface_hub from https://files.pythonhosted.org/packages/72/21/51cddb8850ed3f4dbc21e57c3dabc49e64d5577857ddda7b2eb0ffc2ec0e/huggingface_hub-0.17.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n",
      "Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0239c2d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fd0069c978411ab3cf4063411494bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebeb9ee30684233a2c9a1586f8744fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dece129e37406b9cada638884f6d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1811ca4102354d4b87816bb12550228c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)134a50bb/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928093790fe4e4a9e9ceb09df555041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23206d7a92e2443bb3266f69d1fd8431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a50bb/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eeb29ba1f840328441aab20cd238cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1307fffc3794edca9d99266305f8fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de37676309d4eb8aa0be39df00cf986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30f66d67fbb4e18ba502da8f8403fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1e310b176c4ff29368cb993f234b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-13B-fp16\"#\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be704ef5",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd09c171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model download may failed, please check prior step!\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66131c-5dd3-41b5-8614-6e45e9fed77c",
   "metadata": {},
   "source": [
    "**Rewrite upload module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5716ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer_config.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer_config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/generation_config.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/generation_config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/config.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/special_tokens_map.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/special_tokens_map.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model.bin.index.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model.bin.index.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer.json s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer.model s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer.model\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00003-of-00003.bin s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00003-of-00003.bin\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00001-of-00003.bin s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00001-of-00003.bin\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00002-of-00003.bin s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00002-of-00003.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/Llama-2-13B-fp16/\n",
    "\n",
    "#rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c0a9a",
   "metadata": {},
   "source": [
    "## Prepare a docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48ff3a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "#From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "FROM nvcr.io/nvidia/pytorch:23.02-py3\n",
    "RUN pip3 install sagemaker-training\n",
    "\n",
    "#Remove Cuda 11.8\n",
    "#RUN apt-get -y purge cuda*\n",
    "#RUN apt-get -y autoremove\n",
    "#RUN apt-get -y autoclean\n",
    "#RUN rm -rf /usr/local/cuda*\n",
    "\n",
    "#install Cuda 12\n",
    "#RUN wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\n",
    "#RUN mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "#RUN wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb\n",
    "#RUN dpkg -i cuda-repo-ubuntu2004-12-1-local_12.1.1-530.30.02-1_amd64.deb\n",
    "#RUN cp /var/cuda-repo-ubuntu2004-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "#RUN apt-get update\n",
    "#RUN apt-get -y install cuda\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN update-alternatives --display cuda\n",
    "RUN update-alternatives --auto cuda\n",
    "RUN python3 -m pip uninstall -y deepspeed \n",
    "#RUN python3 -m pip install pytorch-lightning==1.9.0\n",
    "\n",
    "#From requirement.txt\n",
    "RUN python3 -m pip install datasets>=2.8.0\n",
    "RUN python3 -m pip install sentencepiece>=0.1.97\n",
    "RUN python3 -m pip install protobuf==3.20.3\n",
    "RUN python3 -m pip install accelerate>=0.15.0\n",
    "RUN python3 -m pip install torch>=1.12.0\n",
    "RUN python3 -m pip install deepspeed>=0.9.0\n",
    "RUN python3 -m pip install transformers==4.31.0\n",
    "#RUN python3 -m pip install transformers>=4.31.0,!=4.33.2\n",
    "RUN python3 -m pip install tensorboard\n",
    "\n",
    "\n",
    "## Install transfomers version which support LLaMaTokenizer\n",
    "#RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@68d640f7c368bcaaaecfc678f11908ebbd3d6176\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42e8b4c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-east-1\n",
    "!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d1962",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1717f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-llama2-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a814f9d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  113.5MB\n",
      "Step 1/17 : FROM nvcr.io/nvidia/pytorch:23.02-py3\n",
      " ---> 7c3375e220ea\n",
      "Step 2/17 : RUN pip3 install sagemaker-training\n",
      " ---> Using cache\n",
      " ---> c26db60cc80d\n",
      "Step 3/17 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> daf94321439e\n",
      "Step 4/17 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> bd8d2fd7a9a1\n",
      "Step 5/17 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> ff65d34542c3\n",
      "Step 6/17 : RUN update-alternatives --display cuda\n",
      " ---> Using cache\n",
      " ---> e04ac8360543\n",
      "Step 7/17 : RUN update-alternatives --auto cuda\n",
      " ---> Using cache\n",
      " ---> 6c2f3c2edc0f\n",
      "Step 8/17 : RUN python3 -m pip uninstall -y deepspeed\n",
      " ---> Using cache\n",
      " ---> f14a5a8f5df8\n",
      "Step 9/17 : RUN python3 -m pip install datasets>=2.8.0\n",
      " ---> Using cache\n",
      " ---> 6065025ed0e2\n",
      "Step 10/17 : RUN python3 -m pip install sentencepiece>=0.1.97\n",
      " ---> Using cache\n",
      " ---> e9186d65012f\n",
      "Step 11/17 : RUN python3 -m pip install protobuf==3.20.3\n",
      " ---> Using cache\n",
      " ---> ee7e2bcf85e7\n",
      "Step 12/17 : RUN python3 -m pip install accelerate>=0.15.0\n",
      " ---> Using cache\n",
      " ---> 59718b1225de\n",
      "Step 13/17 : RUN python3 -m pip install torch>=1.12.0\n",
      " ---> Using cache\n",
      " ---> 69ef1933c40d\n",
      "Step 14/17 : RUN python3 -m pip install deepspeed>=0.9.0\n",
      " ---> Using cache\n",
      " ---> 8f03265f3d9c\n",
      "Step 15/17 : RUN python3 -m pip install transformers==4.31.0\n",
      " ---> Running in 88b6e07ca1e3\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.31.0\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (0.17.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (1.22.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.31.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.31.0) (1.26.13)\n",
      "Installing collected packages: tokenizers, safetensors, transformers\n",
      "Successfully installed safetensors-0.3.3 tokenizers-0.13.3 transformers-4.31.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 88b6e07ca1e3\n",
      " ---> 5d08c2e8c06c\n",
      "Step 16/17 : RUN python3 -m pip install tensorboard\n",
      " ---> Running in 1177e5c7208e\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.28.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (2.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (65.5.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.51.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard) (1.22.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard) (5.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 1177e5c7208e\n",
      " ---> 94ce99c436a1\n",
      "Step 17/17 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in 8b31d820ebb6\n",
      "Removing intermediate container 8b31d820ebb6\n",
      " ---> add7e22514cd\n",
      "Successfully built add7e22514cd\n",
      "Successfully tagged sagemaker-llama2-13b:latest\n",
      "The push refers to repository [427169985960.dkr.ecr.us-east-1.amazonaws.com/sagemaker-llama2-13b]\n",
      "462fa0b917db: Preparing\n",
      "965fad35d1a3: Preparing\n",
      "21a26dbd9837: Preparing\n",
      "c7f3e28be5a6: Preparing\n",
      "1c23b1f8974a: Preparing\n",
      "b05104af43a8: Preparing\n",
      "00ce1d46ae63: Preparing\n",
      "a80034093396: Preparing\n",
      "1b332cf0a00c: Preparing\n",
      "81a23279aecf: Preparing\n",
      "777f03a73d92: Preparing\n",
      "c395de67fa59: Preparing\n",
      "847e4cb476a3: Preparing\n",
      "362395390e07: Preparing\n",
      "e463f1e12549: Preparing\n",
      "90cfa3fde55b: Preparing\n",
      "b05104af43a8: Waiting\n",
      "05be79be3d00: Preparing\n",
      "1b332cf0a00c: Waiting\n",
      "fa19871c9d97: Preparing\n",
      "9c08be9a4df4: Preparing\n",
      "81a23279aecf: Waiting\n",
      "00ce1d46ae63: Waiting\n",
      "97cfdcfee476: Preparing\n",
      "27d771c824cd: Preparing\n",
      "a80034093396: Waiting\n",
      "777f03a73d92: Waiting\n",
      "38f523d14ea6: Preparing\n",
      "e463f1e12549: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "c395de67fa59: Waiting\n",
      "90cfa3fde55b: Waiting\n",
      "0bb8fce9600a: Preparing\n",
      "4963cc779521: Preparing\n",
      "847e4cb476a3: Waiting\n",
      "05be79be3d00: Waiting\n",
      "362395390e07: Waiting\n",
      "fc6980be3345: Preparing\n",
      "1be3b48a2dd2: Preparing\n",
      "fa19871c9d97: Waiting\n",
      "129b6b66762e: Preparing\n",
      "28a0898660e6: Preparing\n",
      "178f233b6e82: Preparing\n",
      "9c08be9a4df4: Waiting\n",
      "3629a41d267e: Preparing\n",
      "97cfdcfee476: Waiting\n",
      "3c9606098737: Preparing\n",
      "27d771c824cd: Waiting\n",
      "aea7572c6e7c: Preparing\n",
      "28e5b649aa69: Preparing\n",
      "b3eb3f6a881e: Preparing\n",
      "ad36af83523b: Preparing\n",
      "93e86c739007: Preparing\n",
      "2eb589fcc194: Preparing\n",
      "7e114556b94d: Preparing\n",
      "b8390e51490f: Preparing\n",
      "d76bc5715d79: Preparing\n",
      "65f8041c117e: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "2f01e60f0b15: Preparing\n",
      "595350e08a5c: Preparing\n",
      "cf543fc4fccf: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "ad7dc7a9e053: Preparing\n",
      "8067ce044f51: Preparing\n",
      "f8762797e65a: Preparing\n",
      "9c6a2d006077: Preparing\n",
      "de38e00f8e6c: Preparing\n",
      "122ac7e12cc3: Preparing\n",
      "871ecd890743: Preparing\n",
      "0d2b276997c6: Preparing\n",
      "df62fb1402ea: Preparing\n",
      "75e6b8a32bd3: Preparing\n",
      "1f20c057b484: Preparing\n",
      "dd2bc8e06034: Preparing\n",
      "d543b8cad89e: Preparing\n",
      "38f523d14ea6: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "0bb8fce9600a: Waiting\n",
      "4963cc779521: Waiting\n",
      "fc6980be3345: Waiting\n",
      "1be3b48a2dd2: Waiting\n",
      "129b6b66762e: Waiting\n",
      "cf543fc4fccf: Waiting\n",
      "ad7dc7a9e053: Waiting\n",
      "28a0898660e6: Waiting\n",
      "178f233b6e82: Waiting\n",
      "8067ce044f51: Waiting\n",
      "f8762797e65a: Waiting\n",
      "75e6b8a32bd3: Waiting\n",
      "3629a41d267e: Waiting\n",
      "9c6a2d006077: Waiting\n",
      "3c9606098737: Waiting\n",
      "de38e00f8e6c: Waiting\n",
      "122ac7e12cc3: Waiting\n",
      "1f20c057b484: Waiting\n",
      "dd2bc8e06034: Waiting\n",
      "aea7572c6e7c: Waiting\n",
      "871ecd890743: Waiting\n",
      "0d2b276997c6: Waiting\n",
      "b8390e51490f: Waiting\n",
      "df62fb1402ea: Waiting\n",
      "28e5b649aa69: Waiting\n",
      "595350e08a5c: Waiting\n",
      "b3eb3f6a881e: Waiting\n",
      "65f8041c117e: Waiting\n",
      "2f01e60f0b15: Waiting\n",
      "d76bc5715d79: Waiting\n",
      "ad36af83523b: Waiting\n",
      "2eb589fcc194: Waiting\n",
      "7e114556b94d: Waiting\n",
      "93e86c739007: Waiting\n",
      "21a26dbd9837: Layer already exists\n",
      "c7f3e28be5a6: Layer already exists\n",
      "1c23b1f8974a: Layer already exists\n",
      "b05104af43a8: Layer already exists\n",
      "00ce1d46ae63: Layer already exists\n",
      "a80034093396: Layer already exists\n",
      "1b332cf0a00c: Layer already exists\n",
      "81a23279aecf: Layer already exists\n",
      "777f03a73d92: Layer already exists\n",
      "c395de67fa59: Layer already exists\n",
      "847e4cb476a3: Layer already exists\n",
      "362395390e07: Layer already exists\n",
      "90cfa3fde55b: Layer already exists\n",
      "e463f1e12549: Layer already exists\n",
      "05be79be3d00: Layer already exists\n",
      "fa19871c9d97: Layer already exists\n",
      "9c08be9a4df4: Layer already exists\n",
      "97cfdcfee476: Layer already exists\n",
      "27d771c824cd: Layer already exists\n",
      "38f523d14ea6: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "0bb8fce9600a: Layer already exists\n",
      "4963cc779521: Layer already exists\n",
      "fc6980be3345: Layer already exists\n",
      "129b6b66762e: Layer already exists\n",
      "462fa0b917db: Pushed\n",
      "1be3b48a2dd2: Layer already exists\n",
      "28a0898660e6: Layer already exists\n",
      "178f233b6e82: Layer already exists\n",
      "3629a41d267e: Layer already exists\n",
      "3c9606098737: Layer already exists\n",
      "aea7572c6e7c: Layer already exists\n",
      "28e5b649aa69: Layer already exists\n",
      "b3eb3f6a881e: Layer already exists\n",
      "ad36af83523b: Layer already exists\n",
      "7e114556b94d: Layer already exists\n",
      "93e86c739007: Layer already exists\n",
      "2eb589fcc194: Layer already exists\n",
      "b8390e51490f: Layer already exists\n",
      "d76bc5715d79: Layer already exists\n",
      "65f8041c117e: Layer already exists\n",
      "2f01e60f0b15: Layer already exists\n",
      "595350e08a5c: Layer already exists\n",
      "cf543fc4fccf: Layer already exists\n",
      "ad7dc7a9e053: Layer already exists\n",
      "8067ce044f51: Layer already exists\n",
      "f8762797e65a: Layer already exists\n",
      "9c6a2d006077: Layer already exists\n",
      "de38e00f8e6c: Layer already exists\n",
      "871ecd890743: Layer already exists\n",
      "0d2b276997c6: Layer already exists\n",
      "df62fb1402ea: Layer already exists\n",
      "122ac7e12cc3: Layer already exists\n",
      "75e6b8a32bd3: Layer already exists\n",
      "1f20c057b484: Layer already exists\n",
      "dd2bc8e06034: Layer already exists\n",
      "d543b8cad89e: Layer already exists\n",
      "965fad35d1a3: Pushed\n",
      "latest: digest: sha256:9e95f79d9629ad5d82742cc53bc2247e8d4d1dcf3f4e057640355534216b1a60 size: 12943\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} . \n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69f07bc7-a676-4e9d-9bfb-1f5f703d8a5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/DeepSpeedExamples/applications/DeepSpeed-Chat\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8716141c-bb16-4a7a-95e1-ca0471d3b328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting to /bin/systemctl stop docker.service\n",
      "Failed to stop docker.service: The name org.freedesktop.PolicyKit1 was not provided by any .service files\n",
      "See system logs and 'systemctl status docker.service' for details.\n",
      "Warning: Stopping docker.service, but it can still be activated by:\n",
      "  docker.socket\n",
      "Redirecting to /bin/systemctl start docker.service\n",
      "Failed to start docker.service: The name org.freedesktop.PolicyKit1 was not provided by any .service files\n",
      "See system logs and 'systemctl status docker.service' for details.\n"
     ]
    }
   ],
   "source": [
    "!service docker stop\n",
    "!service docker start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4edf0d",
   "metadata": {},
   "source": [
    "### Generate training entrypoint script\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc847d",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9d5507-5e60-4bcb-a0fb-d33700127bde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepSpeedExamples'...\n",
      "remote: Enumerating objects: 8680, done.\u001b[K\n",
      "remote: Counting objects: 100% (1789/1789), done.\u001b[K\n",
      "remote: Compressing objects: 100% (317/317), done.\u001b[K\n",
      "remote: Total 8680 (delta 1581), reused 1523 (delta 1436), pack-reused 6891\u001b[K\n",
      "Receiving objects: 100% (8680/8680), 22.36 MiB | 51.00 MiB/s, done.\n",
      "Resolving deltas: 100% (4910/4910), done.\n"
     ]
    }
   ],
   "source": [
    "!mkdir src\n",
    "!cd src/ && git clone https://github.com/microsoft/DeepSpeedExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39e423-6804-4e4c-b540-1a9e617c7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case the latest version doesn't work, checkout the specific version as of the latest version of 2023-09-25\n",
    "!cd DeepSpeedExamples\n",
    "!git checkout 9b3d8984d8211e87f778da9125c836b3d8c9b90f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198db457-cf2f-47e0-b864-678e230922d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download s5cmd and put it in src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f5267-6a5f-469d-afb9-16dab7bedcd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/peak/s5cmd/releases/download/v2.2.2/s5cmd_2.2.2_Linux-64bit.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37a53a-89f7-43ba-a4e6-8d6437fc1a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -xvf s5cmd_2.2.2_Linux-64bit.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a511ce7d-7ced-47a1-8e35-cdff63bb838f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv s5cmd src/\n",
    "!rm -rf s5cmd_2.2.2_Linux-64bit.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca84c5-cdd9-4fa7-8592-3baa76316749",
   "metadata": {},
   "source": [
    "## Modify some files due to some bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406152c7-5485-4ed6-961e-ce227f25c42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify DeepSpeedChat-on-Amazon-SageMaker/src/DeepSpeedExamples/applications/DeepSpeed-Chat/training/utils/utils.py\n",
    "\n",
    "#Find below function\n",
    "def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
    "    if os.path.exists(model_name_or_path):\n",
    "        # Locally tokenizer loading has some issue, so we need to force download\n",
    "        model_json = os.path.join(model_name_or_path, \"config.json\")\n",
    "        if os.path.exists(model_json):\n",
    "            model_json_file = json.load(open(model_json))\n",
    "            model_name = model_json_file[\"_name_or_path\"]\n",
    "            tokenizer = get_tokenizer(model_name,\n",
    "                                      fast_tokenizer=fast_tokenizer)\n",
    "    else:\n",
    "        tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                  fast_tokenizer=fast_tokenizer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "#####\n",
    "#Replace it into\n",
    "\n",
    "def load_hf_tokenizer(model_name_or_path, fast_tokenizer=True):\n",
    "    tokenizer = get_tokenizer(model_name_or_path,\n",
    "                                  fast_tokenizer=fast_tokenizer)\n",
    "\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b273426-d6f7-43bf-98bd-fbcf6e1567b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.sh\n",
    "#!/bin/bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/Llama-2-13B-fp16/* /tmp/llama-2-13B-fp16/\n",
    "#./s5cmd sync s3://$MODEL_S3_BUCKET/llama-7b-hf/* /tmp/llama-7b-hf/\n",
    "\n",
    "\n",
    "cd DeepSpeedExamples/applications/DeepSpeed-Chat/training/step1_supervised_finetuning/\n",
    "\n",
    "#Fix huggingface issue\n",
    "pip install --upgrade huggingface_hub\n",
    "huggingface-cli login --token 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\n",
    "\n",
    "\n",
    "#bash training_scripts/opt/single_gpu/run_1.3b.sh\n",
    "#!/bin/bash\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# DeepSpeed Team\n",
    "OUTPUT=$1\n",
    "ZERO_STAGE=$2\n",
    "if [ \"$OUTPUT\" == \"\" ]; then\n",
    "    OUTPUT=./output_step1_llama2_7b\n",
    "fi\n",
    "if [ \"$ZERO_STAGE\" == \"\" ]; then\n",
    "    ZERO_STAGE=3\n",
    "fi\n",
    "mkdir -p $OUTPUT\n",
    "\n",
    "deepspeed main.py \\\n",
    "   --data_path Dahoas/synthetic-instruct-gptj-pairwise \\\n",
    "   --data_split 10,0,0 \\\n",
    "   --model_name_or_path /tmp/llama-2-13B-fp16/ \\\n",
    "   --per_device_train_batch_size 2 \\\n",
    "   --per_device_eval_batch_size 2 \\\n",
    "   --max_seq_len 512 \\\n",
    "   --learning_rate 9.65e-6 \\\n",
    "   --weight_decay 0. \\\n",
    "   --num_train_epochs 2  \\\n",
    "   --gradient_accumulation_steps 1 \\\n",
    "   --lr_scheduler_type cosine \\\n",
    "   --num_warmup_steps 0 \\\n",
    "   --seed 1234 \\\n",
    "   --gradient_checkpointing \\\n",
    "   --zero_stage $ZERO_STAGE \\\n",
    "   --deepspeed \\\n",
    "   --output_dir $OUTPUT \\\n",
    "   \n",
    "#   --model_name_or_path TheBloke/Llama-2-13B-fp16 \\\n",
    "#   --model_name_or_path /tmp/Llama-2-13B-fp16/ \\\n",
    "#&> $OUTPUT/training.log\n",
    "\n",
    "#set the timer for stopping the training job\n",
    "#sleep 900\n",
    "#kill \"$!\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "#Upload the trained models to S3\n",
    "#./s5cmd sync ./llama-2-13B-fp16_lora s3://$MODEL_S3_BUCKET/llama-2-13B-fp16_lora\n",
    "\n",
    "#s3://$MODEL_S3_BUCKET/RWKV/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0f06196b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'427169985960.dkr.ecr.us-east-1.amazonaws.com/sagemaker-llama2-13b:latest'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49434d18",
   "metadata": {},
   "source": [
    "**The modified training script**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954cbcf5",
   "metadata": {},
   "source": [
    "Everything is ready, let's launch the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486913ec",
   "metadata": {},
   "source": [
    "## Create SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5922e265-c0b8-445a-b29e-af47ce9a493f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chmod: cannot access ‘lost+found’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod 777 lost+found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e57792db-5f46-499c-bc2d-a7c6252ec2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo chmod -R 777 ./src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69b199e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-25 03:28:16 Starting - Starting the training job...\n",
      "2023-09-25 03:28:24 Downloading - Downloading input data\n",
      "2023-09-25 03:28:24 Training - Training image download completed. Training in progress....\u001b[34m=============\u001b[0m\n",
      "\u001b[34m== PyTorch ==\u001b[0m\n",
      "\u001b[34m=============\u001b[0m\n",
      "\u001b[34mNVIDIA Release 23.02 (build 53420872)\u001b[0m\n",
      "\u001b[34mPyTorch Version 1.14.0a0+44dac51\u001b[0m\n",
      "\u001b[34mContainer image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2014-2023 Facebook Inc.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2011-2013 NYU                      (Clement Farabet)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\u001b[0m\n",
      "\u001b[34mCopyright (c) 2015      Google Inc.\u001b[0m\n",
      "\u001b[34mCopyright (c) 2015      Yangqing Jia\u001b[0m\n",
      "\u001b[34mCopyright (c) 2013-2016 The Caffe contributors\u001b[0m\n",
      "\u001b[34mAll rights reserved.\u001b[0m\n",
      "\u001b[34mVarious files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:58,880 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:58,949 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:59,017 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:59,028 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-427169985960/DeepSpeedChat-SageMaker-Training-2023-09-25-03-28-11-684/source/sourcedir.tar.gz\",\"module_name\":\"train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python38.zip:/usr/lib/python3.8:/usr/lib/python3.8/lib-dynload:/usr/local/lib/python3.8/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train.sh \"\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:59,029 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2023-09-25 03:28:59,029 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/generation_config.json /tmp/llama-2-13B-fp16/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/config.json /tmp/llama-2-13B-fp16/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model.bin.index.json /tmp/llama-2-13B-fp16/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/special_tokens_map.json /tmp/llama-2-13B-fp16/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer.model /tmp/llama-2-13B-fp16/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer.json /tmp/llama-2-13B-fp16/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/tokenizer_config.json /tmp/llama-2-13B-fp16/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00003-of-00003.bin /tmp/llama-2-13B-fp16/pytorch_model-00003-of-00003.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00001-of-00003.bin /tmp/llama-2-13B-fp16/pytorch_model-00001-of-00003.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-427169985960/Llama-2-13B-fp16/pytorch_model-00002-of-00003.bin /tmp/llama-2-13B-fp16/pytorch_model-00002-of-00003.bin\u001b[0m\n",
      "\u001b[34mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.8/dist-packages (0.17.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (1.26.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2022.12.7)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid (permission: read).\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:38,641] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:43,520] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:43,520] [INFO] [runner.py:570:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --data_path Dahoas/synthetic-instruct-gptj-pairwise --data_split 10,0,0 --model_name_or_path /tmp/llama-2-13B-fp16/ --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --max_seq_len 512 --learning_rate 9.65e-6 --weight_decay 0. --num_train_epochs 2 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --gradient_checkpointing --zero_stage 3 --deepspeed --output_dir ./output_step1_llama2_7b\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:45,063] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:163:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:49,312] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,654] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,669] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,669] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,669] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,696] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,715] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,725] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:51,726] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,425] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,563] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,633] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,643] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,657] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,657] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,657] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,659] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:29:56,659] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mYou are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:30:01,321] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 363, num_elems = 13.02B\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/torch/storage.py:315: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.\n",
      "  warnings.warn(message, UserWarning)\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.37s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.42s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.64s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.66s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.68s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.69s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:15,  7.71s/it]#015Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:17,  9.00s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.78s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.78s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.89s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.89s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.89s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.90s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:15<00:07,  7.91s/it]#015Loading checkpoint shards:  67%|██████▋   | 2/3 [00:17<00:08,  8.49s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.04s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.47s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.22s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:19<00:00,  6.61s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.26s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.67s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.26s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.26s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.27s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.27s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.69s/it]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  6.97s/it]#015Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.43s/it]\u001b[0m\n",
      "\u001b[34m#015Downloading metadata:   0%|          | 0.00/1.03k [00:00<?, ?B/s]#015Downloading metadata: 100%|██████████| 1.03k/1.03k [00:00<00:00, 2.09MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data:   0%|          | 0.00/18.2M [00:00<?, ?B/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading data:  23%|██▎       | 4.19M/18.2M [00:00<00:01, 7.85MB/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Downloading data:  69%|██████▉   | 12.6M/18.2M [00:01<00:00, 9.91MB/s]#033[A#015Downloading data: 100%|██████████| 18.2M/18.2M [00:01<00:00, 13.1MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]#015Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34m#015Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]#015Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 2003.97it/s]\u001b[0m\n",
      "\u001b[34m#015Generating train split:   0%|          | 0/33143 [00:00<?, ? examples/s]#015Generating train split: 100%|██████████| 33143/33143 [00:00<00:00, 369578.60 examples/s]\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py38_cu120/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py38_cu120/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py38_cu120 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /usr/local/lib/python3.8/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.575958013534546 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.53493618965149 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.431748628616333 seconds\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 25.933799028396606 seconds\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:34,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:34,983] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.43157958984375 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.531498432159424 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.435086965560913 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.434967517852783 seconds\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,102] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,103] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,103] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,126] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,409] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,410] [INFO] [utils.py:804:see_memory_usage] MA 3.73 GB         Max_MA 4.42 GB         CA 5.72 GB         Max_CA 6 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,410] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.83 GB, percent = 3.6%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,413] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,413] [INFO] [stage3.py:127:__init__] Prefetch bucket size 30000000\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,651] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,651] [INFO] [utils.py:804:see_memory_usage] MA 3.73 GB         Max_MA 3.73 GB         CA 5.72 GB         Max_CA 6 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,651] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.83 GB, percent = 3.6%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 414720 in 81 params\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,917] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,918] [INFO] [utils.py:804:see_memory_usage] MA 3.19 GB         Max_MA 3.76 GB         CA 5.72 GB         Max_CA 6 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:35,918] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.83 GB, percent = 3.6%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:36,162] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:36,163] [INFO] [utils.py:804:see_memory_usage] MA 3.19 GB         Max_MA 3.19 GB         CA 5.72 GB         Max_CA 6 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:36,163] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.83 GB, percent = 3.6%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,140] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,141] [INFO] [utils.py:804:see_memory_usage] MA 3.19 GB         Max_MA 3.19 GB         CA 3.86 GB         Max_CA 6 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,141] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 55.0 GB, percent = 4.9%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,426] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,427] [INFO] [utils.py:804:see_memory_usage] MA 3.19 GB         Max_MA 3.19 GB         CA 3.86 GB         Max_CA 4 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,427] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.82 GB, percent = 3.5%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,686] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,686] [INFO] [utils.py:804:see_memory_usage] MA 9.25 GB         Max_MA 10.41 GB         CA 11.79 GB         Max_CA 12 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,687] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.82 GB, percent = 3.5%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,932] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,933] [INFO] [utils.py:804:see_memory_usage] MA 9.25 GB         Max_MA 9.25 GB         CA 11.79 GB         Max_CA 12 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:40,933] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.82 GB, percent = 3.5%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,198] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,199] [INFO] [utils.py:804:see_memory_usage] MA 21.37 GB         Max_MA 25.11 GB         CA 27.65 GB         Max_CA 28 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,199] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 39.82 GB, percent = 3.5%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,199] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,696] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,697] [INFO] [utils.py:804:see_memory_usage] MA 25.33 GB         Max_MA 25.94 GB         CA 30.74 GB         Max_CA 31 GB \u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,697] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 40.0 GB, percent = 3.6%\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fdc70b9b220>\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[9.65e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdc70b5f760>\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,699] [INFO] [config.py:971:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   fp16_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,700] [INFO] [config.py:971:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   steps_per_print .............. 10\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   train_batch_size ............. 16\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  2\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:31:41,701] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 2, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 100\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": false, \n",
      "        \"output_path\": \"step1_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step1_model_tensorboard\"\n",
      "    }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Evaluating perplexity, Epoch 0/2 *****\u001b[0m\n",
      "\u001b[34mppl: 1990.31884765625\u001b[0m\n",
      "\u001b[34mBeginning of Epoch 1/2, Total Micro Batches 1865\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:38,246] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 3.63s, TFLOPs: 3.65, Samples/sec: 0.55, Time/seq 1.81s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:39,793] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 1.54s, TFLOPs: 8.56, Samples/sec: 1.29, Time/seq 0.77s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:42,011] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.22s, TFLOPs: 5.97, Samples/sec: 0.90, Time/seq 1.11s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:43,552] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 1.54s, TFLOPs: 8.59, Samples/sec: 1.30, Time/seq 0.77s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:45,452] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 1.90s, TFLOPs: 6.97, Samples/sec: 1.05, Time/seq 0.95s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:47,620] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.17s, TFLOPs: 6.10, Samples/sec: 0.92, Time/seq 1.08s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:50,227] [WARNING] [stage3.py:1936:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.08, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:52,872] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.00, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:55,431] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:57,936] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:57,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=6, lr=[9.64997261775981e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:32:57,936] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=7.064143556648067, CurrSamplesPerSec=6.395290333561348, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:00,439] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:03,219] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:06,000] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:08,482] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.33, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:11,315] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.83s, TFLOPs: 4.67, Samples/sec: 0.71, Time/seq 1.42s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:14,132] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.82s, TFLOPs: 4.70, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:16,675] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:19,237] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:21,626] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.39s, TFLOPs: 5.54, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:24,172] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:24,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=6, lr=[9.649664571126915e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:24,173] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=6.497536364225357, CurrSamplesPerSec=6.29236485344852, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:26,861] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.69s, TFLOPs: 4.92, Samples/sec: 0.74, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:29,637] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.77, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:32,151] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.26, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:34,516] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.60, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:37,541] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 3.02s, TFLOPs: 4.37, Samples/sec: 0.66, Time/seq 1.51s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:40,143] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:42,958] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.81s, TFLOPs: 4.70, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:45,452] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.31, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:48,256] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.80s, TFLOPs: 4.72, Samples/sec: 0.71, Time/seq 1.40s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:50,883] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:50,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=6, lr=[9.64901427198597e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:50,884] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=6.309802603173766, CurrSamplesPerSec=6.098924577173604, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:53,527] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:56,030] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:33:58,558] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.24, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:01,311] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:03,938] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:06,445] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:09,204] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.76s, TFLOPs: 4.80, Samples/sec: 0.73, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:12,025] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.82s, TFLOPs: 4.69, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:14,804] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:17,339] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:17,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=6, lr=[9.648021766467953e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:17,340] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=6.241066471035639, CurrSamplesPerSec=6.320629854692749, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:20,038] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.91, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:22,788] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:25,422] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:27,911] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.32, Samples/sec: 0.80, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:30,375] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.37, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:33,084] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.71s, TFLOPs: 4.89, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:35,620] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:38,002] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.38s, TFLOPs: 5.56, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:40,543] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.21, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:42,871] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:42,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=6, lr=[9.64668712497932e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:42,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=6.248358399481105, CurrSamplesPerSec=6.884696059786149, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.33s, TFLOPs: 5.69, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:45,471] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:48,004] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.23, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:50,632] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:53,207] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.14, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:55,875] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.96, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:34:58,372] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.30, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:00,796] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.46, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:03,349] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.18, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:06,116] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.76s, TFLOPs: 4.78, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:08,524] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:08,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=6, lr=[9.645010442196988e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:08,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=6.2478462854001435, CurrSamplesPerSec=6.653813976002239, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:11,026] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:13,764] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.74s, TFLOPs: 4.83, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:16,123] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.61, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:18,656] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.23, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:20,930] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.27s, TFLOPs: 5.82, Samples/sec: 0.88, Time/seq 1.14s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:23,665] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.84, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:26,280] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:29,094] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.81s, TFLOPs: 4.70, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:31,716] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.05, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:34,267] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:34,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=6, lr=[9.64299183706164e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:34,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=6.2442734946792395, CurrSamplesPerSec=6.276667191367888, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.19, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:37,053] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.75, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:39,677] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:42,219] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.21, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:44,913] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.69s, TFLOPs: 4.92, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:47,504] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.11, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:50,217] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.71s, TFLOPs: 4.88, Samples/sec: 0.74, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:52,677] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:55,374] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.69s, TFLOPs: 4.91, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:35:58,011] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.02, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:00,646] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:00,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=6, lr=[9.640631452769279e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:00,647] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=6.22195950177728, CurrSamplesPerSec=6.082326052817163, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.02, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:03,152] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:06,090] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.94s, TFLOPs: 4.51, Samples/sec: 0.68, Time/seq 1.47s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:08,679] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.11, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:11,328] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 5.00, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:14,096] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.78, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:16,573] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.34, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:19,046] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:21,452] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:23,938] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.33, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:26,689] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:26,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=6, lr=[9.637929456761062e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:26,690] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=6.213963362042574, CurrSamplesPerSec=5.82290199745145, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:29,260] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.15, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:31,856] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:34,331] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:36,755] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.46, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:39,608] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.85s, TFLOPs: 4.64, Samples/sec: 0.70, Time/seq 1.43s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:42,195] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:44,979] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:47,331] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.35s, TFLOPs: 5.63, Samples/sec: 0.85, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:49,904] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.15, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:52,683] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:52,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=6, lr=[9.63488604071144e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:52,684] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=6.208835284273873, CurrSamplesPerSec=5.763448144492116, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:55,434] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:36:58,065] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:00,757] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.69s, TFLOPs: 4.92, Samples/sec: 0.74, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:03,421] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.66s, TFLOPs: 4.97, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:06,250] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.83s, TFLOPs: 4.68, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:08,920] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.96, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:11,348] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.45, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:13,949] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:16,444] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.31, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:19,317] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:19,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=6, lr=[9.631501420514545e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:19,318] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=6.190372145076529, CurrSamplesPerSec=5.576121664043909, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.87s, TFLOPs: 4.61, Samples/sec: 0.70, Time/seq 1.44s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:21,820] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:24,591] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.78, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:27,315] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.72s, TFLOPs: 4.86, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:30,067] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:32,717] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 4.99, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:35,319] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:37,986] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.66s, TFLOPs: 4.96, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:40,591] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.08, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:43,168] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.14, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:45,873] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:45,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=6, lr=[9.627775836268883e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:45,874] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=6.176799896217759, CurrSamplesPerSec=5.923353979659188, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.89, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:48,446] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.15, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:51,012] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:53,737] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.72s, TFLOPs: 4.86, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:56,375] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.02, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:37:58,835] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:01,401] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:04,100] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.91, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:06,756] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 4.98, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:09,242] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.32, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:11,747] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:11,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=6, lr=[9.623709552260295e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:11,748] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=6.178029142129207, CurrSamplesPerSec=6.395268393307183, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:14,184] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.43, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:16,913] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.85, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:19,733] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.82s, TFLOPs: 4.69, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:22,304] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.15, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:24,964] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.66s, TFLOPs: 4.98, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:27,371] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:29,700] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.33s, TFLOPs: 5.69, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:32,211] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:34,762] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.19, Samples/sec: 0.78, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:37,067] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:37,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=6, lr=[9.619302856943223e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:37,068] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=6.188605132852173, CurrSamplesPerSec=6.948879260687202, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.74, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:39,687] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.05, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:42,129] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.42, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:44,749] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.05, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:47,562] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.81s, TFLOPs: 4.70, Samples/sec: 0.71, Time/seq 1.41s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:50,174] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.07, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:52,787] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.07, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:55,106] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.71, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:57,641] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:38:59,959] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.71, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:02,312] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:02,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=6, lr=[9.614556062920229e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:02,313] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=6.199046842747098, CurrSamplesPerSec=6.81079670112752, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.35s, TFLOPs: 5.63, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:04,876] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:07,363] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.32, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:09,766] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:12,137] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.37s, TFLOPs: 5.58, Samples/sec: 0.84, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:14,741] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.08, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:17,214] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:19,618] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:22,250] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:24,592] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.34s, TFLOPs: 5.65, Samples/sec: 0.85, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:27,014] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:27,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=6, lr=[9.609469506919832e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:27,015] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=6.216544185942446, CurrSamplesPerSec=6.617521770595204, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.47, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:29,453] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.43, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:31,759] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.74, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:34,059] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.76, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:36,581] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:39,096] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.26, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:41,496] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.52, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:43,988] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.31, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:46,378] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.39s, TFLOPs: 5.54, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:48,699] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.70, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:51,343] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:51,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=6, lr=[9.604043549772622e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:51,344] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=6.237339074534435, CurrSamplesPerSec=6.058279450204767, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:53,752] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:56,206] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.39, Samples/sec: 0.82, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:39:58,632] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.46, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:01,153] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:03,884] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.85, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:06,530] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.00, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:09,097] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:11,543] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.41, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:13,735] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.19s, TFLOPs: 6.04, Samples/sec: 0.91, Time/seq 1.09s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:16,141] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:16,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=6, lr=[9.598278576385654e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:16,142] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=6.249462046370004, CurrSamplesPerSec=6.657021774017713, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:18,638] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.30, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:21,127] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.32, Samples/sec: 0.80, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:23,717] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.11, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:26,199] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.33, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:28,751] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.19, Samples/sec: 0.78, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:31,470] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.72s, TFLOPs: 4.87, Samples/sec: 0.74, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:34,034] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:36,391] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.35s, TFLOPs: 5.62, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:38,948] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.18, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:41,465] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:41,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=6, lr=[9.592174995715153e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:41,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=6.253520430960872, CurrSamplesPerSec=6.365878088193235, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.26, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:43,972] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:46,551] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.13, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:48,911] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.61, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:51,431] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:54,092] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.66s, TFLOPs: 4.97, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:56,606] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:40:59,035] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.45, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:01,600] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:04,129] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.24, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:06,863] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:06,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=6, lr=[9.585733240737496e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:06,863] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=6.256299559980867, CurrSamplesPerSec=5.859552078512865, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.84, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:09,096] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.23s, TFLOPs: 5.93, Samples/sec: 0.90, Time/seq 1.12s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:11,212] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.11s, TFLOPs: 6.26, Samples/sec: 0.95, Time/seq 1.06s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:13,571] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.61, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:16,083] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:18,644] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:21,533] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.89s, TFLOPs: 4.58, Samples/sec: 0.69, Time/seq 1.44s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:24,174] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:26,880] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.89, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:29,193] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.31s, TFLOPs: 5.72, Samples/sec: 0.87, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:31,896] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:31,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=6, lr=[9.578953768418502e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:31,897] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=6.263075308027388, CurrSamplesPerSec=5.927707071800931, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:34,336] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.43, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:36,788] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.40, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:39,192] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:41,648] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.39, Samples/sec: 0.82, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:43,880] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.23s, TFLOPs: 5.93, Samples/sec: 0.90, Time/seq 1.11s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:46,407] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.24, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:48,953] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:51,414] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:53,715] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.75, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:56,132] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:56,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=6, lr=[9.571837059681016e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:56,133] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=6.2782502379117355, CurrSamplesPerSec=6.6269455317278165, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.47, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:41:58,631] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.30, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:00,932] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.75, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:03,268] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.33s, TFLOPs: 5.67, Samples/sec: 0.86, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:05,803] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:08,672] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.87s, TFLOPs: 4.61, Samples/sec: 0.70, Time/seq 1.43s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:11,230] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:13,785] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.18, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:16,256] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.36, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:18,778] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:21,456] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:21,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=6, lr=[9.564383619370789e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:21,457] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=6.280353878607269, CurrSamplesPerSec=5.980686449340652, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:23,940] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.33, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:26,275] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.33s, TFLOPs: 5.67, Samples/sec: 0.86, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:28,854] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.13, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:31,314] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:33,890] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.14, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:36,478] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:38,951] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:41,568] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:44,089] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:46,525] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:46,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=6, lr=[9.556593976220668e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:46,526] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=6.284915924411221, CurrSamplesPerSec=6.576719996722854, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.43, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:48,932] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.50, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:51,507] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.14, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:53,655] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.15s, TFLOPs: 6.16, Samples/sec: 0.93, Time/seq 1.07s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:56,269] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:42:58,807] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:01,255] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.41, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:03,684] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.45, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:06,642] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.96s, TFLOPs: 4.47, Samples/sec: 0.68, Time/seq 1.48s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:08,815] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.17s, TFLOPs: 6.09, Samples/sec: 0.92, Time/seq 1.09s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:11,554] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:11,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=6, lr=[9.54846868281309e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:11,554] [INFO] [timer.py:260:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=6.289518340945019, CurrSamplesPerSec=5.848818511688858, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.74s, TFLOPs: 4.83, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:13,974] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.47, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:16,556] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.13, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:19,239] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.93, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:21,924] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.93, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:24,387] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:27,254] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.86s, TFLOPs: 4.62, Samples/sec: 0.70, Time/seq 1.43s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:29,635] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.38s, TFLOPs: 5.56, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:32,181] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:34,631] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.41, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:37,272] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:37,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=6, lr=[9.540008315540885e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:37,273] [INFO] [timer.py:260:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=6.287179253688197, CurrSamplesPerSec=6.064872002916538, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:39,834] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:42,286] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.40, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:45,248] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.96s, TFLOPs: 4.47, Samples/sec: 0.68, Time/seq 1.48s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:47,581] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.33s, TFLOPs: 5.67, Samples/sec: 0.86, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:50,208] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:53,130] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.92s, TFLOPs: 4.53, Samples/sec: 0.69, Time/seq 1.46s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:55,451] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.70, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:43:57,895] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.42, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:00,407] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:02,620] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:02,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=6, lr=[9.531213474566375e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:02,621] [INFO] [timer.py:260:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=6.2884000018483555, CurrSamplesPerSec=7.240426582816842, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.21s, TFLOPs: 5.98, Samples/sec: 0.90, Time/seq 1.11s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:05,410] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.79s, TFLOPs: 4.74, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:07,935] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.24, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:10,467] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.23, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:13,027] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:15,691] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.66s, TFLOPs: 4.97, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:17,999] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.31s, TFLOPs: 5.74, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:20,632] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:23,211] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.13, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:25,919] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.71s, TFLOPs: 4.89, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:28,439] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:28,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=6, lr=[9.522084783778817e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:28,440] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=6.285339173059345, CurrSamplesPerSec=6.357730974468792, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:30,704] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.26s, TFLOPs: 5.84, Samples/sec: 0.88, Time/seq 1.13s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:33,200] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.31, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:35,779] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.13, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:38,315] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:40,863] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:43,153] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.29s, TFLOPs: 5.78, Samples/sec: 0.87, Time/seq 1.14s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:45,932] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.76, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:48,446] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.26, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:50,872] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.46, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:53,659] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:53,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=6, lr=[9.512622890750135e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:53,660] [INFO] [timer.py:260:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=6.28765674686037, CurrSamplesPerSec=5.74955356886597, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.78s, TFLOPs: 4.75, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:56,199] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.21, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:44:58,829] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:01,351] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:04,093] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.74s, TFLOPs: 4.83, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:06,771] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:09,492] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.72s, TFLOPs: 4.87, Samples/sec: 0.74, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:12,024] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.23, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:14,635] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.07, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:17,155] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:19,771] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:19,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=6, lr=[9.502828466688984e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:19,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=6.282435243212211, CurrSamplesPerSec=6.122008001529661, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:22,332] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:24,800] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.36, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:27,576] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.77, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:29,943] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.59, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:32,584] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:35,228] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:37,908] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:40,583] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.95, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:43,144] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:45,815] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:45,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=6, lr=[9.492702206393134e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:45,816] [INFO] [timer.py:260:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=6.278130706431724, CurrSamplesPerSec=5.998179154012717, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.96, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:48,229] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.49, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:50,914] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.93, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:53,423] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.28, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:56,019] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:45:58,248] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.23s, TFLOPs: 5.94, Samples/sec: 0.90, Time/seq 1.11s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:00,620] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.37s, TFLOPs: 5.58, Samples/sec: 0.84, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:03,292] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.95, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:05,891] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:08,306] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.48, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:10,901] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:10,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=6, lr=[9.482244828200194e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:10,902] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=6.281504907333462, CurrSamplesPerSec=6.172272314852839, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:13,661] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.76s, TFLOPs: 4.80, Samples/sec: 0.73, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:16,175] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:18,791] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:21,627] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.83s, TFLOPs: 4.67, Samples/sec: 0.71, Time/seq 1.42s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:24,222] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:26,807] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:29,204] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.52, Samples/sec: 0.84, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:31,561] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.62, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:34,264] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:36,855] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:36,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=6, lr=[9.471457073936633e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:36,856] [INFO] [timer.py:260:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=6.278147964638829, CurrSamplesPerSec=6.183565535017855, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.11, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:39,146] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.29s, TFLOPs: 5.78, Samples/sec: 0.87, Time/seq 1.14s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:42,064] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.92s, TFLOPs: 4.54, Samples/sec: 0.69, Time/seq 1.46s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:44,312] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.24s, TFLOPs: 5.89, Samples/sec: 0.89, Time/seq 1.12s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:46,715] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:49,301] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:52,135] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.83s, TFLOPs: 4.67, Samples/sec: 0.71, Time/seq 1.42s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:54,619] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.33, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:57,244] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:46:59,946] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:02,487] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:02,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=6, lr=[9.460339708865182e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:02,488] [INFO] [timer.py:260:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=6.277349169042248, CurrSamplesPerSec=6.304159669754514, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.21, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:05,064] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.14, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:07,826] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.76s, TFLOPs: 4.79, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:10,269] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.42, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:12,596] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.69, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:14,919] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.70, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:17,571] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 4.99, Samples/sec: 0.75, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:19,972] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:22,769] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.79s, TFLOPs: 4.73, Samples/sec: 0.72, Time/seq 1.40s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:25,491] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.72s, TFLOPs: 4.86, Samples/sec: 0.74, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:27,961] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:27,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=6, lr=[9.448893521630524e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:27,962] [INFO] [timer.py:260:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=6.277732020365407, CurrSamplesPerSec=6.488010271164942, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.36, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:30,700] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.74s, TFLOPs: 4.83, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:33,462] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.76s, TFLOPs: 4.79, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:35,940] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.34, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:38,645] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.89, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:41,323] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:43,944] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.05, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:46,643] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:49,398] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:52,129] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.85, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:54,897] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:54,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=6, lr=[9.437119324203362e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:54,898] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=6.2680282796979725, CurrSamplesPerSec=5.785360608811966, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.78, Samples/sec: 0.72, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:47:57,671] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.77s, TFLOPs: 4.77, Samples/sec: 0.72, Time/seq 1.39s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:00,300] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:03,013] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.71s, TFLOPs: 4.88, Samples/sec: 0.74, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:05,923] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.91s, TFLOPs: 4.55, Samples/sec: 0.69, Time/seq 1.45s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:08,399] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:10,887] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.48s, TFLOPs: 5.32, Samples/sec: 0.80, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:13,448] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.17, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:16,129] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:18,351] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.22s, TFLOPs: 5.96, Samples/sec: 0.90, Time/seq 1.11s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:20,968] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:20,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=6, lr=[9.425017951822819e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:20,968] [INFO] [timer.py:260:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=6.264623792786864, CurrSamplesPerSec=6.1228167879146635, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.61s, TFLOPs: 5.06, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:23,368] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.52, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:25,760] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.39s, TFLOPs: 5.54, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:28,270] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:30,970] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:33,567] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:35,939] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.37s, TFLOPs: 5.58, Samples/sec: 0.84, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:38,279] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.34s, TFLOPs: 5.66, Samples/sec: 0.86, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:40,823] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.20, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:43,294] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.36, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:45,995] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:45,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=6, lr=[9.412590262937183e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:45,995] [INFO] [timer.py:260:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=6.268173884817573, CurrSamplesPerSec=5.931993681272323, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.70s, TFLOPs: 4.90, Samples/sec: 0.74, Time/seq 1.35s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:48,628] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:51,278] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 4.99, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:53,879] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:56,511] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:48:59,113] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.09, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:01,718] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.60s, TFLOPs: 5.08, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:04,402] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.93, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:07,092] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.69s, TFLOPs: 4.92, Samples/sec: 0.74, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:09,579] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.32, Samples/sec: 0.80, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:12,318] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:12,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=6, lr=[9.399837139143018e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:12,318] [INFO] [timer.py:260:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=6.263336713001626, CurrSamplesPerSec=5.850811289615266, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.74s, TFLOPs: 4.83, Samples/sec: 0.73, Time/seq 1.37s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:14,693] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.37s, TFLOPs: 5.58, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:17,044] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.35s, TFLOPs: 5.63, Samples/sec: 0.85, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:19,468] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.46, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:22,018] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.19, Samples/sec: 0.78, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:24,316] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.76, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:26,953] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.02, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:29,108] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.15s, TFLOPs: 6.14, Samples/sec: 0.93, Time/seq 1.08s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:31,530] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.42s, TFLOPs: 5.47, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:33,907] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.37s, TFLOPs: 5.57, Samples/sec: 0.84, Time/seq 1.19s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:36,251] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:36,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=6, lr=[9.386759485122612e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:36,252] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=6.273495281021165, CurrSamplesPerSec=6.835963337591579, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.34s, TFLOPs: 5.65, Samples/sec: 0.85, Time/seq 1.17s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:38,791] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.21, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:41,189] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.52, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:43,692] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:46,316] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.04, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:48,779] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.37, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:51,375] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.59s, TFLOPs: 5.10, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:53,850] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.35, Samples/sec: 0.81, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:56,148] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.76, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:49:58,416] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.27s, TFLOPs: 5.84, Samples/sec: 0.88, Time/seq 1.13s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:00,871] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:00,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=6, lr=[9.373358228579816e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:00,871] [INFO] [timer.py:260:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=6.279043158397294, CurrSamplesPerSec=6.526702210937133, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.45s, TFLOPs: 5.39, Samples/sec: 0.82, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:03,312] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.44s, TFLOPs: 5.42, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:06,385] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 3.07s, TFLOPs: 4.31, Samples/sec: 0.65, Time/seq 1.54s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:08,969] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:11,879] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.91s, TFLOPs: 4.55, Samples/sec: 0.69, Time/seq 1.45s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:14,382] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:16,743] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.36s, TFLOPs: 5.60, Samples/sec: 0.85, Time/seq 1.18s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:19,316] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.57s, TFLOPs: 5.15, Samples/sec: 0.78, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:21,853] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:24,417] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.56s, TFLOPs: 5.16, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:26,948] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:26,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=6, lr=[9.359634320174225e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:26,948] [INFO] [timer.py:260:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=6.275728866286358, CurrSamplesPerSec=6.333207472348725, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.53s, TFLOPs: 5.23, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:29,702] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.75s, TFLOPs: 4.81, Samples/sec: 0.73, Time/seq 1.38s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:31,995] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.29s, TFLOPs: 5.77, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:34,616] [WARNING] [stage3.py:1936:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.62s, TFLOPs: 5.05, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:37,246] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:39,923] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.68s, TFLOPs: 4.94, Samples/sec: 0.75, Time/seq 1.34s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:42,651] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.85, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:45,061] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.49, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:47,570] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:49,887] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.32s, TFLOPs: 5.71, Samples/sec: 0.86, Time/seq 1.16s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:52,387] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:52,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=6, lr=[9.345588733453738e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:52,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=6.2762313337759785, CurrSamplesPerSec=6.407253695356138, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.50s, TFLOPs: 5.29, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:54,911] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.52s, TFLOPs: 5.25, Samples/sec: 0.79, Time/seq 1.26s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:57,215] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.30s, TFLOPs: 5.75, Samples/sec: 0.87, Time/seq 1.15s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:50:59,674] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.38, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:02,344] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.67s, TFLOPs: 4.96, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:04,745] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.40s, TFLOPs: 5.51, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:07,255] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.51s, TFLOPs: 5.27, Samples/sec: 0.80, Time/seq 1.25s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:09,684] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.43s, TFLOPs: 5.45, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:12,173] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.49s, TFLOPs: 5.32, Samples/sec: 0.80, Time/seq 1.24s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:14,805] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.63s, TFLOPs: 5.03, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:17,607] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:17,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=6, lr=[9.331222464785504e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:17,608] [INFO] [timer.py:260:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=6.277975761670194, CurrSamplesPerSec=5.716166531248608, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.80s, TFLOPs: 4.72, Samples/sec: 0.71, Time/seq 1.40s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:20,415] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.80s, TFLOPs: 4.72, Samples/sec: 0.71, Time/seq 1.40s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:22,606] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.19s, TFLOPs: 6.04, Samples/sec: 0.91, Time/seq 1.09s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:25,015] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.41s, TFLOPs: 5.49, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:27,671] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.65s, TFLOPs: 4.98, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:30,400] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.73s, TFLOPs: 4.85, Samples/sec: 0.73, Time/seq 1.36s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:33,043] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.64s, TFLOPs: 5.01, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:35,627] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:38,211] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.58s, TFLOPs: 5.12, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:40,683] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.47s, TFLOPs: 5.36, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:43,619] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:43,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=6, lr=[9.31653653328524e-06], mom=[(0.9, 0.95)]\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:43,620] [INFO] [timer.py:260:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=6.275285096758281, CurrSamplesPerSec=5.457017635304371, MemAllocated=25.46GB, MaxMemAllocated=31.07GB\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.93s, TFLOPs: 4.51, Samples/sec: 0.68, Time/seq 1.47s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:45,824] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.20s, TFLOPs: 6.01, Samples/sec: 0.91, Time/seq 1.10s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:48,362] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.54s, TFLOPs: 5.22, Samples/sec: 0.79, Time/seq 1.27s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:50,919] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.55s, TFLOPs: 5.18, Samples/sec: 0.78, Time/seq 1.28s, Batch Size: 2, Sequence Length: 512\u001b[0m\n",
      "\u001b[34m[2023-09-25 03:51:53,377] [WARNING] [stage3.py:1936:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34mModel Parameters: 12.852 B, Latency: 2.46s, TFLOPs: 5.39, Samples/sec: 0.81, Time/seq 1.23s, Batch Size: 2, Sequence Length: 512\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 28\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#instance_type = 'ml.g5.12xlarge'\u001b[39;00m\n\u001b[1;32m     15\u001b[0m estimator \u001b[38;5;241m=\u001b[39m Estimator(role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m     16\u001b[0m                       entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.sh\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m                       source_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./src\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m                       keep_alive_period_in_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m\n\u001b[1;32m     26\u001b[0m                       )\n\u001b[0;32m---> 28\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# estimator.fit(inputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1310\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1310\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2580\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2580\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:4849\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4829\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4830\u001b[0m \n\u001b[1;32m   4831\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4847\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4848\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4849\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboto_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sagemaker/session.py:6710\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   6708\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 6710\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   6713\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'DeepSpeedChat-SageMaker-Training'\n",
    "\n",
    "\n",
    "#instance_type = 'ml.p3dn.24xlarge'\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "#instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='train.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      keep_alive_period_in_seconds=3600\n",
    "                      )\n",
    "\n",
    "estimator.fit()\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb8296-2eee-4bee-bb97-8417dc5af1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
